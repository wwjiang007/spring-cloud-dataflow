[[configuration]]
= Server Configuration
[partintro]
--
In this section you will learn how to configure Spring Cloud Data Flow server's features such as the relational database to use and security.
--
[[enable-disable-specific-features]]
== Feature Toggles

Data Flow server offers specific set of features that can be enabled/disabled when launching. These features include all the lifecycle operations, REST endpoints (server, client implementations including Shell and the UI) for:

. Streams
. Tasks
. Analytics

One can enable, disable these features by setting the following boolean properties when launching the Data Flow server:

* `spring.cloud.dataflow.features.streams-enabled`
* `spring.cloud.dataflow.features.tasks-enabled`
* `spring.cloud.dataflow.features.analytics-enabled`

By default, all the features are enabled.
Note: Since analytics feature is enabled by default, the Data Flow server is expected to have a valid Redis store available as analytic repository as we provide a default implementation of analytics based on Redis.
      This also means that the Data Flow server's `health` depends on the redis store availability as well.
      If you do not want to enabled HTTP endpoints to read analytics data written to Redis, then disable the analytics feature using the property mentioned above.

The REST endpoint `/features` provides information on the features enabled/disabled.

[[configuration-rdbms]]
== Database Configuration

Spring Cloud Data Flow provides schemas for H2, HSQLDB, MySQL, Oracle, Postgresql, DB2 and SqlServer that will be automatically created when the server starts.

The JDBC drivers for *MySQL* (via MariaDB driver), *HSQLDB*, *PostgreSQL* along with embedded *H2* are available out of the box.
If you are using any other database, then the corresponding JDBC driver jar needs to be on the classpath of the server.

The database properties can be passed as command-line arguments to the Data Flow Server.

For instance,
If you are using *MySQL*:

[source,bash]
----
java -jar spring-cloud-dataflow-server-local/target/spring-cloud-dataflow-server-local-1.0.0.BUILD-SNAPSHOT.jar \
    --spring.datasource.url=jdbc:mysql:<db-info> \
    --spring.datasource.username=<user> \
    --spring.datasource.password=<password> \
    --spring.datasource.driver-class-name=org.mariadb.jdbc.Driver &
----

For *PostgreSQL*:

[source,bash]
----
java -jar spring-cloud-dataflow-server-local/target/spring-cloud-dataflow-server-local-1.0.0.BUILD-SNAPSHOT.jar \
    --spring.datasource.url=jdbc:postgresql:<db-info> \
    --spring.datasource.username=<user> \
    --spring.datasource.password=<password> \
    --spring.datasource.driver-class-name=org.postgresql.Driver &
----

For *HSQLDB*:

[source,bash]
----
java -jar spring-cloud-dataflow-server-local/target/spring-cloud-dataflow-server-local-1.0.0.BUILD-SNAPSHOT.jar \
    --spring.datasource.url=jdbc:hsqldb:<db-info> \
    --spring.datasource.username=SA \
    --spring.datasource.driver-class-name=org.hsqldb.jdbc.JDBCDriver &
----

NOTE: There is a schema update to the Spring Cloud Data Flow datastore when
upgrading from version `1.0.x` to `1.1.x`.  Migration scripts for specific
database types can be found
https://github.com/spring-cloud/spring-cloud-task/tree/master/spring-cloud-task-core/src/main/resources/org/springframework/cloud/task/migration[here].

NOTE: If you wish to use an external H2 database instance instead of the one
embedded with Spring Cloud Data Flow set the
`spring.dataflow.embedded.database.enabled` property to false.  If
`spring.dataflow.embedded.database.enabled` is set to false or a database
other than h2 is specified as the datasource the embedded database will not
start.

[[configuration-security]]
== Security

By default, the Data Flow server is unsecured and runs on an unencrypted HTTP connection.
You can secure your REST endpoints, as well as the Data Flow Dashboard by enabling HTTPS
and requiring clients to authenticate using either:

* http://oauth.net/2/[OAuth 2.0]
* Basic Authentication

NOTE:
By default, the REST endpoints (administration, management and health), as well
as the Dashboard UI do not require authenticated access.

[[configuration-security-enabling-https]]
=== Enabling HTTPS

By default, the dashboard, management, and health endpoints use HTTP as a transport.
You can switch to HTTPS easily, by adding a certificate to your configuration in
`application.yml`.

[source,yaml]
----
server:
  port: 8443                                         # <1>
  ssl:
    key-alias: yourKeyAlias                          # <2>
    key-store: path/to/keystore                      # <3>
    key-store-password: yourKeyStorePassword         # <4>
    key-password: yourKeyPassword                    # <5>
    trust-store: path/to/trust-store                 # <6>
    trust-store-password: yourTrustStorePassword     # <7>
----

<1> As the default port is `9393`, you may choose to change the port to a more common HTTPs-typical port.
<2> The alias (or name) under which the key is stored in the keystore.
<3> The path to the keystore file. Classpath resources may also be specified, by using the classpath prefix: `classpath:path/to/keystore`
<4> The password of the keystore.
<5> The password of the key.
<6> The path to the truststore file. Classpath resources may also be specified, by using the classpath prefix: `classpath:path/to/trust-store`
<7> The password of the trust store.

NOTE: If HTTPS is enabled, it will completely replace HTTP as the protocol over
which the REST endpoints and the Data Flow Dashboard interact. Plain HTTP requests
will fail - therefore, make sure that you configure your Shell accordingly.

[[configuration-security-self-signed-certificates]]
==== Using Self-Signed Certificates

For testing purposes or during development it might be convenient to create self-signed certificates.
To get started, execute the following command to create a certificate:

[source,bash]
----
$ keytool -genkey -alias dataflow -keyalg RSA -keystore dataflow.keystore \
          -validity 3650 -storetype JKS \
          -dname "CN=localhost, OU=Spring, O=Pivotal, L=Kailua-Kona, ST=HI, C=US"  # <1>
          -keypass dataflow -storepass dataflow
----

<1> _CN_ is the only important parameter here. It should match the domain you are trying to access, e.g. `localhost`.

Then add the following to your `application.yml` file:

[source,yaml]
----
server:
  port: 8443
  ssl:
    enabled: true
    key-alias: dataflow
    key-store: "/your/path/to/dataflow.keystore"
    key-store-type: jks
    key-store-password: dataflow
    key-password: dataflow
----

This is all that's needed for the Data Flow Server. Once you start the server,
you should be able to access it via https://localhost:8443/[https://localhost:8443/].
As this is a self-signed certificate, you will hit a warning in your browser, that
you need to ignore.

[[configuration-security-self-signed-certificates-shell]]
==== Self-Signed Certificates and the Shell

By default self-signed certificates are an issue for the Shell and additional steps
are necessary to make the Shell work with self-signed certificates. Two options
are available:

1. Add the self-signed certificate to the JVM truststore
2. Skip certificate validation

**Add the self-signed certificate to the JVM truststore**

In order to use the JVM truststore option, we need to
export the previously created certificate from the keystore:

[source,bash]
----
$ keytool -export -alias dataflow -keystore dataflow.keystore -file dataflow_cert -storepass dataflow
----

Next, we need to create a truststore which the Shell will use:

[source,bash]
----
$ keytool -importcert -keystore dataflow.truststore -alias dataflow -storepass dataflow -file dataflow_cert -noprompt
----

Now, you are ready to launch the Data Flow Shell using the following JVM arguments:

[source,bash,subs=attributes]
----
$ java -Djavax.net.ssl.trustStorePassword=dataflow \
       -Djavax.net.ssl.trustStore=/path/to/dataflow.truststore \
       -Djavax.net.ssl.trustStoreType=jks \
       -jar spring-cloud-dataflow-shell-{project-version}.jar
----

[TIP]
====
In case you run into trouble establishing a connection via SSL, you can enable additional
logging by using and setting the `javax.net.debug` JVM argument to `ssl`.
====

Don't forget to target the Data Flow Server with:

[source,bash]
----
dataflow:> dataflow config server https://localhost:8443/
----

**Skip Certificate Validation**

Alternatively, you can also bypass the certification validation by providing the
optional command-line parameter `--dataflow.skip-ssl-validation=true`.

Using this command-line parameter, the shell will accept any (self-signed) SSL
certificate.

[WARNING]
====
If possible you should avoid using this option. Disabling the trust manager
defeats the purpose of SSL and makes you vulnerable to man-in-the-middle attacks.
====

[[configuration-security-basic-authentication]]
=== Basic Authentication

https://en.wikipedia.org/wiki/Basic_access_authentication[Basic Authentication] can
be enabled by adding the following to `application.yml` or via
environment variables:

[source,yaml]
----
security:
  basic:
    enabled: true                                                     # <1>
    realm: Spring Cloud Data Flow                                     # <2>
----

<1> Enables basic authentication. Must be set to true for security to be enabled.
<2> (Optional) The realm for Basic authentication. Will default to _Spring_ if not explicitly set.

NOTE: Current versions of Chrome do not display the _realm_. Please see the following
https://bugs.chromium.org/p/chromium/issues/detail?id=544244[Chromium issue ticket] for more information.

In this use-case, the underlying Spring Boot will auto-create a user called _user_
with an auto-generated password which will be printed out to the console upon startup.

.Default Spring Boot user credentials
image::{dataflow-asciidoc}/images/dataflow-security-default-user.png[Default Spring Boot user credentials , scaledwidth="100%"]

NOTE: Please be aware of inherent issues of Basic Authentication and _logging out_, since the credentials are cached by the browser and simply browsing back to application pages will log you back in.

If you need to define more than one file-based user account, please take a look at
_File based authentication_.

[[configuration-security-file-based-authentication]]
==== File based authentication

By default Spring Boot allows you to only specify one single user. Spring Cloud
Data Flow also supports the listing of more than one user in a configuration file,
as described below. Each user must be assigned a password and one or more roles:

[source,yaml]
----
security:
  basic:
    enabled: true
    realm: Spring Cloud Data Flow
spring:
  cloud:
    dataflow:
      security:
        authentication:
          file:
            enabled: true                                                 # <1>
            users:                                                        # <2>
              bob: bobspassword, ROLE_MANAGE                              # <3>
              alice: alicepwd, ROLE_VIEW, ROLE_CREATE
----

<1> Enables file based authentication
<2> This is a yaml map of username to password
<3> Each map `value` is made of a corresponding password and role(s), comma separated

IMPORTANT: As of Spring Cloud Data Flow 1.1, roles are not supported, yet (specified roles are ignored). Due to an https://github.com/spring-projects/spring-security/issues/3403[issue] in
Spring Security, though, at least one role must be provided.

[[configuration-security-ldap-authentication]]
==== LDAP Authentication

Spring Cloud Data Flow also supports authentication against an LDAP server (Lightweight Directory Access Protocol), providing support for the following 2 modes:

* Direct bind
* Search and bind

When the LDAP authentication option is activated, the default single user mode is
turned off.

In _direct bind mode_, a pattern is defined for the user’s distinguished name (DN),
using a placeholder for the username. The authentication process derives the
distinguished name of the user by replacing the placeholder and use it to authenticate
a user against the LDAP server, along with the supplied password. You can set up
LDAP direct bind as follows:

[source,yaml]
----
security:
  basic:
    enabled: true
    realm: Spring Cloud Data Flow
spring:
  cloud:
    dataflow:
      security:
        authentication:
          ldap:
            enabled: true                                                 # <1>
            url: ldap://ldap.example.com:3309                             # <2>
            userDnPattern: uid={0},ou=people,dc=example,dc=com            # <3>
----

<1> Enables LDAP authentication
<2> The URL for the LDAP server
<3> The distinguished name (DN) pattern for authenticating against the server

The _search and bind_ mode involves connecting to an LDAP server, either anonymously
or with a fixed account, and searching for the distinguished name of the authenticating
user based on its username, and then using the resulting value and the supplied password
for binding to the LDAP server. This option is configured as follows:

[source,yaml]
----
security:
  basic:
    enabled: true
    realm: Spring Cloud Data Flow
spring:
  cloud:
    dataflow:
      security:
        authentication:
          ldap:
            enabled: true                                                 # <1>
            url: ldap://localhost:10389                                   # <2>
            managerDn: uid=admin,ou=system                                # <3>
            managerPassword: secret                                       # <4>
            userSearchBase: ou=otherpeople,dc=example,dc=com              # <5>
            userSearchFilter: uid={0}                                     # <6>
----

<1> Enables LDAP integration
<2> The URL of the LDAP server
<3> A DN for to authenticate to the LDAP server, if anonymous searches are not supported (optional, required together with next option)
<4> A password to authenticate to the LDAP server, if anonymous searches are not supported (optional, required together with previous option)
<5> The base for searching the DN of the authenticating user (serves to restrict the scope of the search)
<6> The search filter for the DN of the authenticating user

TIP: For more information, please also see the chapter
http://docs.spring.io/spring-security/site/docs/current/reference/html/ldap.html[LDAP Authentication]
of the Spring Security reference guide.

===== LDAP Transport Security

When connecting to an LDAP server, you typically (In the LDAP world) have 2 options
in order to establish a connection to an LDAP server securely:

* LDAP over SSL (LDAPs)
* Start Transport Layer Security (Start TLS is defined in https://www.ietf.org/rfc/rfc2830.txt[RFC2830])

As of _Spring Cloud Data Flow 1.1.0_ only LDAPs is supported out-of-the-box. When using
official certificates no special configuration is necessary, in order to connect
to an LDAP Server via LDAPs. Just change the url format to **ldaps**, e.g. `ldaps://localhost:636`.

In case of using self-signed certificates, the setup for your Spring Cloud Data Flow
server becomes slightly more complex. The setup is very similar to
<<configuration-security-self-signed-certificates>> (Please read first) and
Spring Cloud Data Flow needs to reference a _trustStore_ in order to work with
your self-signed certificates.

IMPORTANT: While useful during development and testing, please never use
self-signed certificates in production!

Ultimately you have to provide a set of system properties to reference
the trustStore and its credentials when starting the server:

[source,bash,subs=attributes]
----
$ java -Djavax.net.ssl.trustStorePassword=dataflow \
       -Djavax.net.ssl.trustStore=/path/to/dataflow.truststore \
       -Djavax.net.ssl.trustStoreType=jks \
       -jar spring-cloud-starter-dataflow-server-local-{project-version}.jar
----

As mentioned above, another option to connect to an LDAP server securely is via _Start TLS_.
In the LDAP world, LDAPs is technically even considered deprecated in favor of Start TLS. However,
this option is currently not supported out-of-the-box by Spring Cloud Data Flow.

Please follow the following https://github.com/spring-cloud/spring-cloud-dataflow/issues/963[issue
tracker ticket] to track its implementation. You may also want to look at the
Spring LDAP reference documentation chapter on
http://docs.spring.io/spring-ldap/docs/current/reference/#custom-dircontext-authentication-processing[Custom DirContext Authentication Processing] for further details.

[[customizing-authorization]]
==== Customizing authorization

All of the above deals with authentication, _i.e._ how to assess the identity of the user. Irrespective of the option chosen, you can also customize *authorization* _i.e._
who can do what.

The default scheme uses three roles to protect the xref:api-guide[REST endpoints]
that Spring Cloud Data Flow exposes:

* *ROLE_VIEW* for anything that relates to retrieving state
* *ROLE_CREATE* for anything that involves creating, deleting or mutating the state of the system
* *ROLE_MANAGE* for boot management endpoints.

All of those defaults are specified in `dataflow-server-defaults.yml` which is
part of the Spring Cloud Data Flow Core Module. Nonetheless, you can
override those, if desired, e.g. in `application.yml`. The configuration takes
the form of a YAML *list* (as some rules may have precedence over others) and so
you'll need to copy/paste the whole list and tailor it to your needs (as there is no way to merge lists). Always refer to your version of `application.yml`, as the snippet reproduced
below may be out-dated. The default rules are as such:

[source,yaml]
----
spring:
  cloud:
    dataflow:
      security:
        authorization:
          enabled: true
          rules:
            # Metrics

            - GET    /metrics/**                     => hasRole('ROLE_VIEW')
            - DELETE /metrics/**                     => hasRole('ROLE_CREATE')

            # Boot Endpoints

            - GET    /management/**                  => hasRole('ROLE_MANAGE')

            # Apps

            - GET    /apps                           => hasRole('ROLE_VIEW')
            - GET    /apps/**                        => hasRole('ROLE_VIEW')
            - DELETE /apps/**                        => hasRole('ROLE_CREATE')
            - POST   /apps                           => hasRole('ROLE_CREATE')
            - POST   /apps/**                        => hasRole('ROLE_CREATE')

            # Completions

            - GET /completions/**                    => hasRole('ROLE_CREATE')

            # Job Executions & Batch Job Execution Steps && Job Step Execution Progress

            - GET    /jobs/executions                => hasRole('ROLE_VIEW')
            - PUT    /jobs/executions/**             => hasRole('ROLE_CREATE')
            - GET    /jobs/executions/**             => hasRole('ROLE_VIEW')

            # Batch Job Instances

            - GET    /jobs/instances                 => hasRole('ROLE_VIEW')
            - GET    /jobs/instances/*               => hasRole('ROLE_VIEW')

            # Running Applications

            - GET    /runtime/apps                   => hasRole('ROLE_VIEW')
            - GET    /runtime/apps/**                => hasRole('ROLE_VIEW')

            # Stream Definitions

            - GET    /streams/definitions            => hasRole('ROLE_VIEW')
            - GET    /streams/definitions/*          => hasRole('ROLE_VIEW')
            - GET    /streams/definitions/*/related  => hasRole('ROLE_VIEW')
            - POST   /streams/definitions            => hasRole('ROLE_CREATE')
            - DELETE /streams/definitions/*          => hasRole('ROLE_CREATE')
            - DELETE /streams/definitions            => hasRole('ROLE_CREATE')

            # Stream Deployments

            - DELETE /streams/deployments/*          => hasRole('ROLE_CREATE')
            - DELETE /streams/deployments            => hasRole('ROLE_CREATE')
            - POST   /streams/deployments/*          => hasRole('ROLE_CREATE')

            # Task Definitions

            - POST   /tasks/definitions              => hasRole('ROLE_CREATE')
            - DELETE /tasks/definitions/*            => hasRole('ROLE_CREATE')
            - GET    /tasks/definitions              => hasRole('ROLE_VIEW')
            - GET    /tasks/definitions/*            => hasRole('ROLE_VIEW')

            # Task Executions

            - GET    /tasks/executions               => hasRole('ROLE_VIEW')
            - GET    /tasks/executions/*             => hasRole('ROLE_VIEW')
----

The format of each line is the following:
----
HTTP_METHOD URL_PATTERN '=>' SECURITY_ATTRIBUTE
----

where

* HTTP_METHOD is one http method, capital case
* URL_PATTERN is an Ant style URL pattern
* SECURITY_ATTRIBUTE is a SpEL expression (see http://docs.spring.io/spring-security/site/docs/current/reference/htmlsingle/#el-access)
* each of those separated by one or several blank characters (spaces, tabs, _etc._)

Be mindful that the above is indeed a YAML list, not a map (thus the use of '-' dashes at the start of each line) that lives under the `spring.cloud.dataflow.security.authorization.rules` key.

[TIP]
====
In case you are solely interested in authentication but not authorization,
for instance every user shall have have access to all endpoints, then you can also
set `spring.cloud.dataflow.security.authorization.enabled=false`.
====

[[authorization-shell-and-dashboard]]
==== Authorization - Shell and Dashboard Behavior

When authorization is enabled, the _Dashboard_ and the _Shell_ will be _role-aware_,
meaning that depending on the assigned role(s), not all functionality may be visible.

For instance, Shell commands, for which the user does not have the necessary roles
for, will be marked as unavailable.

[IMPORTANT]
====
Currently, the Shell's `help` command will list commands that are unavailable.
Please track the following issue: https://github.com/spring-projects/spring-shell/issues/115
====

Similarly for the _Dashboard_, the UI will not show pages, or page elements, for
which the user is not authorized for.

[[ldap-authorization-and-roles]]
==== Authorization with Ldap

When configuring Ldap for authentication, you can also specify the `group-role-attribute`
in conjunction with `group-search-base` and `group-search-filter`.

The _group role attribure_ contains the name of the role. If not specified, the
`ROLE_MANAGE` role is populated by default.

For further information, please refer to http://docs.spring.io/spring-security/site/docs/current/reference/htmlsingle/#loading-authorities[Configuring an LDAP Server] of the Spring Security reference guide.

[[configuration-security-oauth2]]
=== OAuth 2.0

http://oauth.net/2/[OAuth 2.0] allows you to integrate Spring Cloud
Data Flow into Single Sign On (SSO) environments. The following 2 OAuth2 Grant Types will be used:

* _Authorization Code_ - Used for the GUI (Browser) integration. You will be redirected to your OAuth Service for authentication
* _Password_ - Used by the shell (And the REST integration), so you can login using username and password

The REST endpoints are secured via Basic Authentication but will use the Password
Grand Type under the covers to authenticate with your OAuth2 service.

NOTE: When authentication is set up, it is strongly recommended to enable HTTPS
as well, especially in production environments.

You can turn on OAuth2 authentication by adding the following to `application.yml` or via
environment variables:

[source,yaml]
----
security:
  basic:
    enabled: true                                                     # <1>
    realm: Spring Cloud Data Flow                                     # <2>
  oauth2:                                                             # <3>
    client:
      client-id: myclient
      client-secret: mysecret
      access-token-uri: http://127.0.0.1:9999/oauth/token
      user-authorization-uri: http://127.0.0.1:9999/oauth/authorize
    resource:
      user-info-uri: http://127.0.0.1:9999/me
----

<1> Must be set to `true` for security to be enabled.
<2> The realm for Basic authentication
<3> OAuth Configuration Section, if you leave off the OAuth2 section, Basic Authentication will be enabled instead.

NOTE: As of the current version, Spring Cloud Data Flow does not provide
finer-grained authorization when OAUTH is used as authentication mechanism. Thus,
once you are logged in, you have full access to all functionality.

You can verify that basic authentication is working properly using _curl_:

[source,bash]
----
$ curl -u myusername:mypassword http://localhost:9393/
----

As a result you should see a list of available REST endpoints.

==== Authentication using the Spring Cloud Data Flow Shell

If your OAuth2 provider supports the _Password_ Grant Type you can start the
_Data Flow Shell_ with:

[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-shell-{project-version}.jar \
  --dataflow.uri=http://localhost:9393 \
  --dataflow.username=my_username --dataflow.password=my_password
----

NOTE: Keep in mind that when authentication for Spring Cloud Data Flow is enabled,
the underlying OAuth2 provider *must* support the _Password_ OAuth2 Grant Type,
if you want to use the Shell.

From within the Data Flow Shell you can also provide credentials using:

[source,bash]
----
dataflow config server --uri http://localhost:9393 --username my_username --password my_password
----

Once successfully targeted, you should see the following output:

[source,bash]
----
dataflow:>dataflow config info
dataflow config info

╔═══════════╤═══════════════════════════════════════╗
║Credentials│[username='my_username, password=****']║
╠═══════════╪═══════════════════════════════════════╣
║Result     │                                       ║
║Target     │http://localhost:9393                  ║
╚═══════════╧═══════════════════════════════════════╝
----
==== OAuth2 Authentication Examples

===== Local OAuth2 Server

With http://projects.spring.io/spring-security-oauth/[Spring Security OAuth] you
can easily create your own OAuth2 Server with the following 2 simple annotations:

* @EnableResourceServer
* @EnableAuthorizationServer

A working example application can be found at:

https://github.com/ghillert/oauth-test-server/[https://github.com/ghillert/oauth-test-server/]

Simply clone the project, built and start it. Furthermore configure Spring Cloud
Data Flow with the respective _Client Id_ and _Client Secret_.

===== Authentication using GitHub

If you rather like to use an existing OAuth2 provider, here is an example for GitHub.
First you need to **Register a new application** under your GitHub account at:

https://github.com/settings/developers[https://github.com/settings/developers]

When running a default version of Spring Cloud Data Flow locally, your GitHub configuration
should look like the following:

.Register an OAuth Application for GitHub
image::{dataflow-asciidoc}/images/dataflow-security-github.png[Register an OAuth Application for GitHub , scaledwidth="100%"]

NOTE: For the _Authorization callback URL_ you will enter Spring Cloud Data Flow's Login URL, e.g. `http://localhost:9393/login`.

Configure Spring Cloud Data Flow with the GitHub relevant Client Id and Secret:

[source,yaml]
----
security:
  basic:
    enabled: true
  oauth2:
    client:
      client-id: your-github-client-id
      client-secret: your-github-client-secret
      access-token-uri: https://github.com/login/oauth/access_token
      user-authorization-uri: https://github.com/login/oauth/authorize
    resource:
      user-info-uri: https://api.github.com/user
----

IMPORTANT: GitHub does not support the OAuth2 password grant type. As such you cannot use the Spring Cloud Data Flow Shell in conjunction with GitHub.

=== Securing the Spring Boot Management Endpoints

When enabling security, please also make sure that the http://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-monitoring.html[Spring Boot HTTP Management Endpoints]
are secured as well. You can enable security for the management endpoints by adding the following to `application.yml`:

[source,yaml]
----
management:
  contextPath: /management
  security:
    enabled: true
----

IMPORTANT: If you don't explicitly enable security for the management endpoints,
you may end up having unsecured REST endpoints, despite `security.basic.enabled`
being set to `true`.

[[configuration-monitoring-management]]
== Monitoring and Management
The Spring Cloud Data Flow server is a Spring Boot application that includes the http://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#production-ready[Actuator
library], which adds several production ready features to help you monitor and manage your application.

The Actuator library adds http endpoints under the context path `/management` that is also
a discovery page for available endpoints.  For example, there is a `health` endpoint
that shows application health information and an `env` that lists properties from
Spring's `ConfigurableEnvironment`.  By default only the health and application info
endpoints are accessible.  The other endpoints are considered to be _sensitive_
and need to be http://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#production-ready-customizing-endpoints[enabled explicitly via configuration].  If you are enabling
_sensitive_ endpoints then you should also
<<configuration-security,secure the Data Flow server's endpoints>> so that
information is not inadvertently exposed to unauthenticated users.  The local Data Flow server has security disabled by default, so all actuator endpoints are available.

The Data Flow server requires a relational database and if the feature toggled for
analytics is enabled, a Redis server is also required.  The Data Flow server will
autoconfigure the https://github.com/spring-projects/spring-boot/blob/v1.4.1.RELEASE/spring-boot-actuator/src/main/java/org/springframework/boot/actuate/health/DataSourceHealthIndicator.java[DataSourceHealthIndicator] and https://github.com/spring-projects/spring-boot/blob/v1.4.1.RELEASE/spring-boot-actuator/src/main/java/org/springframework/boot/actuate/health/RedisHealthIndicator.java[RedisHealthIndicator] if needed.  The health of these two services is incorporated to the overall health status of the server through the `health` endpoint.

=== Spring Boot Admin
A nice way to visualize and interact with actuator endpoints is to incorporate the
https://github.com/codecentric/spring-boot-admin[Spring Boot Admin] client library into the Spring Cloud Data Flow server.  You can create the Spring Boot Admin application by following
http://codecentric.github.io/spring-boot-admin/1.4.3/#set-up-admin-server[a few simple steps].

An easy way to include the client library into the Data Flow server is to create a
new Data Flow Server project from http://start.spring.io.  Type 'flow' in the
"Search for dependencies" text box and select the server runtime you want to customize.
A simple way to have the Spring Cloud Data Flow server be a client to the Spring Boot
Admin Server is by adding a dependency to the Data Flow server's pom and an additional
configuration property as documented in http://codecentric.github.io/spring-boot-admin/1.4.3/#register-clients-via-spring-boot-admin[Registering Client Applications].

This will result in a UI with tabs for each of the actuator endpoints.

.Spring Boot Admin UI
image::{dataflow-asciidoc}/images/spring-boot-admin.png[Spring Boot Admin UI, scaledwidth="80%"]

Additional configuration is required to interact with JMX beans and logging levels. Refer
to the Spring Boot admin documentation for more information.  As only the `info`
and `health` endpoints are available to unauthenticated users, you should enable security on
the Data Flow Server and also http://codecentric.github.io/spring-boot-admin/1.4.3/#_securing_spring_boot_admin_server[configure Spring Boot Admin server's security] so that it
can securely access the actuator endpoints.

=== Monitoring Deployed Applications

The applications that are deployed by Spring Cloud Data Flow are based on Spring Boot which
contains several features for monitoring your application in production.  Each deployed
application contains http://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html[several web endpoints] for monitoring and interacting with Stream and Task applications.

In particular, the metrics endpoint contains counters
and gauges for HTTP requests, http://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-metrics.html#production-ready-system-metrics[System Metrics] (such as JVM stats), http://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-metrics.html#production-ready-datasource-metrics[DataSource Metrics] and http://docs.spring.io/spring-integration/reference/htmlsingle/#mgmt-channel-features[Message Channel Metrics] (such as message rates).  In turn, these metrics can be exported periodically to various application monitoring tools via
http://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-metrics.html#production-ready-metric-writers[MetricWriter] implementations.  You can control how often and which Spring Boot metrics are exported through the use of include and exclude name filters.

The project https://github.com/spring-cloud/spring-cloud-dataflow-metrics[Spring Cloud Data Flow Metrics] provides the foundation for
exporting Spring Boot metrics.  The main project provides Spring Boots AutoConfiguration to setup the exporting process and common
functionality such as defining a metric name prefix appropriate for your environement.  For example, you may want to include the
region where the application is running in addition to the application's name and stream/task to which it belongs.
The main project also includes a `LogMetricWriter` so that
metrics can be stored into the log file.  While very simple in approach, log files are often ingested into application monitoring tools
(such as Splunk) where they can be further processed to create dashboards of an application's performance.

The project https://github.com/spring-cloud/spring-cloud-dataflow-metrics-datadog[Spring Cloud Data Flow Metrics Datadog Metrics] provides
integration to export Spring Boot metrics to https://www.datadoghq.com/[Datadog].

To make use of this functionality, you will need to add additional dependencies into your Stream and Task applications.  To customize
the "out of the box" Task and Stream applications you can use the http://start-scs.cfapps.io/[Data Flow Initializr] to generate a
project and then add to the generated Maven pom file the MetricWriter implementation you want to use.  The documentation on the
Data Flow Metrics project pages provides the additional information you need to get started.











